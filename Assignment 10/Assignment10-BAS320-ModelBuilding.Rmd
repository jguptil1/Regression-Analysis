---
title: "BAS 320 - Assignment 10 - Model Building"
author: "Jack Guptill"
output:
  word_document: default
---

************************************************************************************

[//]:   (Submission notes: a knitted version of the assignment must be uploaded to Canvas before the deadline along with the raw .Rmd.  I recommend knitting to a Word document, adding any desired extra content or formatting, and submitting that or the Word document that has been saved as a PDF.  Late work is not accepted.)

[//]:   (Instructions for Assignment 10:  follow the instructions on Canvas.  This chunk of code is what you can use to read in data files. If you're working with data in `BAS320datasetsFall23.RData` and `BAS320datasetsYCategoricalFall23.RData` you can uncomment the line.  Otherwise, you'll want to come up with names for your data frames and add some `read.csv` commands here (illustrated with a few examples in the R chunk below).  Many lines of code in the examples will need to be uncommented and run for it to knit properly )

```{r Data Wrangling}
library(regclass)
getwd()  #This should output the path to your BAS folder; change with Session, Set Working Directory if not

#Either use the new .RData file for this assignment or find your own dataset!
#load("BAS320datasetsFall23.RData") 
#load("BAS320datasetsYCategoricalFall23.RData")
GAReg <- read.csv("GamesAttendanceReg.csv",stringsAsFactors=TRUE) #Data for Regression Model
GSLog = read.csv('GamesStandingsLog.csv', stringsAsFactors=TRUE) #Data for Logistic Model

#Dataset requirements:

# Set 1:  Y is categorical (2 levels; you may be able to create this if your data doesn't have it)
# * between 4-20 X's (NULL out columns to get down to 20 if necessary)
# * any categorical predictors should be between 2-12ish levels (you can combine some to get it down that low if needed)
# * between 300-1000 rows (you'll be able to take a random sample to get it down to 1000 rows if necessary)

# Set 2:  Y is numeric
# * between 4-20 X's (NULL out columns to get down to 20 if necessary)
# * any categorical predictors should be between 2-12ish levels (you can combine some to get it down that low if needed)
# * between 300-1000 rows (you'll be able to take a random sample to get it down to 1000 rows if necessary)



# WRANGLING STEP 1
# NULL out predictors that you don't want to use (meaningless columns, identifier
# variables, columns with a lot of missing values, redundant columns, etc.)
# Aim for between 4-20 predictors!


#Set 1 Transformations
#summary(GALog)
GSLog = GSLog[GSLog$week %in% c(1:17),]
GSLog$week = as.integer(GSLog$week)
GSLog$week =  as.integer(GSLog$week)
GSLog$tie= as.factor(GSLog$tie)
GSLog$year_y <- NULL
GSLog$simple_rating <- NULL
GSLog$tie <- NULL
GSLog$winner <- NULL
GSLog$margin_of_victory <- NULL
GSLog$points_differential <- NULL
GSLog$wins <-NULL #taking out as it shows end of season cumulative wins
GSLog$loss <- NULL #taking out as it shows end of season cumulative losses
GSLog$home_team <-NULL
GSLog$away_team <-NULL
#GSLog$X <- NULL

#Set 2 Transformations

GAReg = GAReg[GAReg$week_x %in% c(1:17),]
GAReg$week_x = as.integer(GAReg$week_x)
GAReg$winner <- NULL
GAReg$tie <- NULL
GAReg$home_team <-NULL
GAReg$away_team <-NULL
GAReg$X <-NULL


#WRANGLING STEP 2:  if you have a categorical variable you'd like to use but it has "too many" levels, you can uncomment the following lines of code (after adapting it with the name of your columns and dataset and uncommenting it) to get it to a target number of levels.  It basically combines all the least common levels of the categorical variable into a new level called "Combined", leaving with you the desired number of levels.  Run a summary() though to make sure because if there are ties for frequencies, it may leave you with more or fewer than the target!

#target <- 3
#threshold <- sort(table(DATA$categoricalvariable),decreasing=TRUE)[target]
#DATA$newcatvariable <- combine_rare_levels(DATA$categoricalvariable,threshold)$values
#summary(DATA$newcatvariable)
#If your levels have spaces, dashes, or any other characters, go ahead and rename them
#levels(DATA$newcatvariable) <-  paste("Level",1:nlevels(DATA$newcatvariable),sep="")



#WRANGLING STEP 3:  Your dataset may have missing values.  It's a good idea to exclude these for this analysis (though there are much better ways to handle them).  Adapt the following code to your data.
#summary(GSLog)  #check for NAs

GSLog <- droplevels(GSLog[complete.cases(GSLog),])
GAReg <- droplevels(GAReg[complete.cases(GAReg),])


#WRANGLING STEP 4:  If your dataset is much bigger than 1000 rows, you can take a random sample of them; adapt the code to your dataframe to reduce it to 1000 rows (or smaller if you want)

SmallLog <- GSLog[ sample(1:nrow(GSLog),size=1000), ]
SmallReg <- GAReg[ sample(1:nrow(GAReg),size=1000), ]

#WRANGLING STEP 5: 
#If you have a numeric Y that you'd like to convert into a two-level categorical Y (e.g. High vs. Low), you can uncomment and adapt the following lines of code. The code below basically converts values into "High" or "Low" depending on if the number is greater than 3 or not.  You'll have to change data frame names, column names, and the "3" into whatever you feel is appropriate.  If you do this, ensure that the equivalent of mean(DATA$newY=="High") is between 0.20 and 0.80; if not try changing the numeric cutoff into something else

# DATA$newY <- factor( ifelse( DATA$numericalx > 3, "High","Low") )
# mean(DATA$newY=="High")
# DATA$numericalx <- NULL  #Delete the variable that categories came from

#WRANGLING STEP 6:  
# * If you use the MOVIEDATA in BAS320datasetsYCategoricalFall23.RData, make sure you are NOT predicting Outcome from Outcome2 or Outcome3, etc.  Once you've selected your Y variable, NULL out the the other 2 possible Outcome columns)
# * If you use the BEER data in BAS320datasetsYCategoricalFall23.RData, make sure you define a Y variable with levels High and Low for just one of the review scores, then NULL OUT the other review scores.


#WRANGLING STEP 7:  
#Run summary() on your data to make sure you don't have any missing values and that
#all categorical predictors have between 2-12ish levels.  You might be combining some
#levels later on.


```

************************************************************************************

The following gives a skeleton/template that you could fill out to narrate your analysis.  You'll need to adapt words throughout to make it specific to your dataset.  Feel free to deviate from the skeleton as long as you're hitting all the required points (see Rubric and detailed list of requirements on Canvas)!  Delete this paragraph (and any prompts provided in other paragraphs) before knitting/submitting.  The document should flow nicely as a professional report.  

Code you should keep:  output of `build_model` (remember to add `prompt=FALSE` once you're happy with it before knitting), output of `BM$CVtable` (assuming `build_model` was left-arrowed into an object named BM), contents of `S` (object created by `step` with `trace=0`).

Code you should delete after knitting: code reading in the data, nulling out columns, taking subsets, combining categories, creating categorical variables from numerical ones if you used it; basically everything in the R chunk above this where you do data wrangling (and any other code you created that isn't requested).


## Part 1:  Descriptive model for if the Home Team will Win a Football Game

I'm using logistic regression to predict whether or not a football team that is home will win a football game. For this logistic regression I am predicting that they will win the football game (Yes level) from prior games data. This data includes predictors such as the year, what week in the season it is, the points of the winning team, the points lost by the winning team, the amount of yards gained by the winning team, the turnovers by the winning team, the yards lost by the home team, the turnovers lost by the home team, the total points for the home team in a season, the total points against the home team in a season, the strength of the home teams schedule, their offensive ranking, their defensive ranking, if they have been in the playoffs, or if they are a superbowl winner. 

I'm making this model primarily out of curiosity and to see how the predictor variables interact to make a prediction for the home team winning.

The best model included the amount of points against the home team, the offensive ranking of the home team,the yards lost by the home team, if the team made the playoffs, how many turnovers the team had, the interaction between offensive ranking and yards lost, the interaction of offensive ranking and turnovers lost, and finally the interaction between yards lost and turnovers lost. 

It is important to note that this model doesn't hold much practical significance due to having to need to know some variables information that you would only know after a game concluded. This model is purely build to see from a descriptive perspective how the predictor variables interact and which predictor variables are best to explain the home team winning. 

The data I'm using comes from kaggle, however a lot of data transformations and joins were done personally with python. The data has a total of 5104 rows with 18 total variables and 17 predictor variables. 

Kaggle Link: https://www.kaggle.com/datasets/sujaykapadnis/nfl-stadium-attendance-dataset
Github: https://github.com/jguptil1/Portfolio-Projects/tree/main/FootballStandings

```{r Part 1,warning=FALSE}
#Adapt the step code to your data; if ~.^2 doesn't work, try ~. instead or try to NULL out some predictors you don't find interesting.

naive <- glm(home_team_winner~1,data=SmallLog,family=binomial)
full <- glm(home_team_winner~.^2,data=SmallLog,family=binomial) 


#You can ignore warnings(), which most likely will be about fitted probabilities numerically 0 or 1 occurred


#BEFORE KNITTING because you don't want this output in the document
#S <- step(naive,scope=list(lower=naive,upper=full),direction="both",trace=1)



# Call:  glm(formula = home_team_winner ~ points_against + offensive_ranking + 
#     yds_loss + playoffs + turnovers_loss + offensive_ranking:yds_loss + 
#     offensive_ranking:turnovers_loss + yds_loss:turnovers_loss, 
#     family = binomial, data = SmallLog)

#Have this step() command be knitted in your document instead
S <- step(naive,scope=list(lower=naive,upper=full),direction="both",trace=0)
S
```

## Part 2:  Predictive model for the Points of the Winning Team

I'm using linear regression to predict what the points will be for the winning team using the year, what week in the season it is, the points of the winning team, the points lost by the winning team, the amount of yards gained by the winning team, the turnovers by the winning team, the yards lost by the home team, the turnovers lost by the home team, the total points for the home team in a season, the total points against the home team in a season, the strength of the home teams schedule, their offensive ranking, their defensive ranking, if they have been in the playoffs, or if they are a Superbowl winner. 

I'm making this model because I am genuinely curious to how you would predict the points of the winning team by knowing the other variables. This could have applications for sports betting or knowing how well your team will do in a given game.

The data I'm using comes from kaggle, however a lot of data transformations and joins were done personally with python. The data has a total of 5104 rows with 18 total variables and 17 predictor variables. 

Kaggle Link: https://www.kaggle.com/datasets/sujaykapadnis/nfl-stadium-attendance-dataset
Github: https://github.com/jguptil1/Portfolio-Projects/tree/main/FootballStandings

The best predictors that ended up in the model is the home team winning the game, the year of the game, the points lost by the team, yards won by the winning team, the turnovers by the winning team, the yards lost, hte turnovers lost, the total points for the team in a season, the total points against the team in a season, and if the team made it to the Superbowl in a given season. With these ten predictor variables the estimated generalized error of the model from K-fold cross validation is 6.01. The RMSE of the holdout was ~6.65 meaning that there is a 10.52% increase in error in the holdout. This tells us that the model is not great at predicting new data as the percentage of error is greater than 10% for hold out data. 


```{r Part 2, warning=FALSE}
#Create your training data and holdout data (adapt this code to your data and needs)
#If you have 700+ rows, have your training set be 500 rows and keep size=500
#Otherwise, have size=0.7*nrow(DATA)

SmallLog$day <- NULL

set.seed(320); train.rows <- sample(1:nrow(SmallLog),size=500)
TRAIN2 <- SmallLog[train.rows,]
HOLDOUT2 <- SmallLog[-train.rows,]
BM <- build_model(pts_win~.,data=TRAIN2,type="predictive",seed=320,holdout=HOLDOUT2, prompt=FALSE)

BM$CVtable

6.645071/6.012168 #ratio of RMSE Holdout ot Estimated Generalized Error


```


