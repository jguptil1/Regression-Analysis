---
title: "BAS 320 - Assignment 7 - Multiple Regression Part 2"
author: "Jack Guptill"
output:
  word_document: default
---


************************************************************************************

```{r reading in data for assignment2}
library(regclass)
setwd('C:/Users/jackg/OneDrive - University of Tennessee/UTK Fall 23/BAS 320/Homework Files')  #This should output the path to your BAS folder; change with Session, Set Working Directory if not

load("BAS320datasetsFall23.RData")  #Note updated .RData from from Assignment 2


#Remember that whatever data you choose needs:
# * One Y variable
# * At least 3 numeric Xs but no more than 8 Xs
# * No categorical predictors
# * In the model you fit, at least 2 X's NEED TO BE STATISTICALLY SIGNIFICANT

#NOTE 1:  if your dataset has categorical predictors, null them out!  Only numeric predictors allowed here
#DATA$categoricalpredictor <- NULL

#NOTE 2:  if you have more than 1000 datapoints, take a random sample of 1000 (otherwise
#check_regression may not work)
#set.seed(320); DATA3 <- DATA2[ sample(1:nrow(DATA2),size=1000), ]


###################################################################################

BEER = read.csv('C:/Users/jackg/OneDrive - University of Tennessee/UTK Fall 23/BAS 320/Homework Files/Assignment 7/beer_profile_and_ratings.csv')

BEER = BEER[,c(6:25)]
set.seed(880); BEER2 <- BEER[ sample(1:nrow(BEER),size=1000), ] #taking a sample of 1000 rows
#head(BEER2)
M = lm(review_aroma~.,data=BEER2)
drop1(M, test="F")


#good columns = spices, Hoppy, review_taste, Bitter, Sour, Body

BEER2 = BEER2[, c('Spices', 'Hoppy', 'review_taste', 'Bitter', 'Sour', 'Body', 'review_aroma')]
###################################################################################


#NOTE 3:  if you have a dataset with a LOT of predictors, restrict yourself to the "top 8"
#To identify which are the top 8, one way is:
#1)  Fit the full model:  M <- lm(y~.,data=DATA)
#2)  Run drop1(M,test="F")
#3)  The top 8 will have the largest 8 F values

```

***********************************************************************************


## Analysis of

I'm using a multiple regression model to predict the review that a beer might get for it's aroma. The model to predict this aroma review will use the amount of spices, hops, bitterness, sourness, the body of the beer, and the overall review of the taste as predictors.

I'm making this model primarily out of curiosity to see how plausible it is to predict something so subjective such as the aroma. I am also curious to the interactions of the predictors that will be taking place in the model. 

The data I'm using comes from kaggle (https://www.kaggle.com/datasets/ruthgn/beer-profile-and-ratings-data-set/data?select=beer_profile_and_ratings.csv) and contains 3197 rows with 25 variables. The specific subset that I will be using to build the model has 1000 rows and 7 variables. 


## Investigation of the relationship between ... (replace with Y) and ... (replace with an X)

After running a simple linear relationship, looking at the relationship between the aroma review and the sourness, it seems to me that a polynomial model would improve the R squared value due to the non linear relationship trend that has developed. After analyzing the various orders to which the model could use, it seems that the third order would be the best increase the r^2 value from roughly 9% to a little less than 11%. While not a significant amount of increase to the R^2 value, it is nonetheless increasing it. Order three can be seen in the visualization as the green line with black being a zero order polynomial model. 

```{r polynomial model}
#Finding the most non-linear predictor variable
#good columns = spices, Hoppy, review_taste, Bitter, Sour, Body
#M = lm(review_aroma~Spices, data=BEER2); choose_order(M)
#M = lm(review_aroma~Hoppy, data=BEER2); choose_order(M)
#M = lm(review_aroma~review_taste, data=BEER2); choose_order(M)
#M = lm(review_aroma~Bitter, data=BEER2); choose_order(M)
M = lm(review_aroma~Sour, data=BEER2); choose_order(M) #order three is the best
#M = lm(review_aroma~Body, data=BEER2); choose_order(M)


```

## Multiple regression model and checking of assumptions

After examining the output in testing the assumptions for a multiple regression model, the full model passes the tests for linearity when it came to the hops, taste review, bitterness, and sourness predictors. The model failed linearity for the spices and body predictor variables. It is important to note that linearity for the overall model failed due to a lack of duplicate values. If the model was more robust in overall sample size and a different ransom sample were to be taken, this condition might have passed. In the output it also states that we have failed both in equal spread and normality of the residuals. Due to our sample size being at least 25, we can further inspect the output graphs to override these decisions. Starting with the far left residual plot, it doesn't seem like we are violating linearity which confirms most of the test output. It also seems that the equal spread condition isn't grossly violated in any way. Data points seem to be roughly equal by eye. For the QQ plot it seems that we have minor peeling on bottom edge. We do however have skew at the top end of the graph. Based on this and my own opinion, I think the model does exhibit a reasonable reflection of reality due to the tests not being grossly violated in anyway. 

```{r multiple regression model}
M =lm(review_aroma ~ Spices+Hoppy+review_taste+Bitter+Sour+Body,data=BEER2)
summary(M)
check_regression(M,extra=TRUE)
```

## Identification of influential points

(In this section, you'll run the model through `influence_plot`.  Include the plot and a list of row numbers that gave the influential points.  If this is a LONG list, edit it down post knitting to just have a line of numbers)

(Describe how many influential points are in your regression.  Influential points are poorly predicted by the model, so identify *why* at least one of these points is influential, i.e., if they are very unusual in terms of one predictor variable or a combination of predictor variables. I'd recommend printing out an influential row and perhaps a summary of the column(s) in which its unusual)

For this model, we have 10 points of influence that contain both high leverage and a large deleted studentized residual. The specific row numbers can be found below and these data points are negatively impacted the overall model due to their overall influence. I want to focus on The 387th beer. It is is strange in the fact that has a low amount of hops, spice, overall taste review, sourness, and body, but it has a high bitterness and overall aroma score making it a point of influence. Please see the data frame bellow that outlays the percentile to which the predictors are for this beer. 

```{r influence plot}
influence_plot(M)



influential.rows <- influence_plot(M)$Leverage
INFLUENCE <- data.frame( matrix(0,nrow=length(influential.rows),ncol=ncol(BEER2)) )
names(INFLUENCE) <- names(BEER2)

for (r in 1:length(influential.rows)) {
  x <- as.numeric(BEER2[influential.rows[r],])
  INFLUENCE[r,] <- sapply(1:length(x),function(i) { mean(BEER2[[i]] <= x[i])} )
}
round(INFLUENCE,digits=2)
```

## Investigation of an interaction between Hoppiness and the review of the taste

I want to investigate the interaction of various levels of hoppiness for a beer and how the review taste impacts the overall aroma of the beer. 

The overall regression equation for this interaction is as follows: 

review aroma = .410 + .881*review_taste - .006*Hoppy + .001*review_taste*Hoppy

For beers with the 25th percentile of Hoppy (19): 
review aroma = .296 + .9*review_taste

For beers with the 75th percentile of Hoppy (59): 
review aroma = .056 + .94*review_taste

As the review of the taste increases we see an increase in the aroma review. We see this as an overall positive relationship. The rate at which it increases however, is changed with how hoppy the beer is. With a hooppier beer, the rate is much stronger than a less hoppy beer. A "medium" beer in terms of hoppiness sits right in between the two. Knowing this, we can assume that there is an interaction between how hoppy the beer is and the aroma that a beer will get from the review of the taste.   

For beers that have a relatively low amount of Hoppiness there seems to be a good amount of variation in the review of the aroma. As we get towards the median value of hoppiness (33) the variation widens and it looks to be getting wider as the hoppiness of a beer increases. 



```{r interaction}
#You'll need to run the next 2 lines, but not include them in your report
#M <- lm(review_aroma ~ .^2, data=BEER2) # do all 2 way interactions
#see_interactions(M,cex=0.6,pos="topleft",many=TRUE)
#This is what we want to see in your report

M <- lm(review_aroma ~ review_taste*Hoppy, data=BEER2)
summary(M)

visualize_model(M, cex=0.6,pos="topleft",many=TRUE)

#summary(BEER2$Hoppy)

```










